# -Reinforcement-Learning-Facility-Layout-Optimization
# ê°•í™”í•™ìŠµ ê¸°ë°˜ ìƒì‚° ì„¤ë¹„ ë°°ì¹˜ ìµœì í™” (Reinforcement Learning Facility Layout Optimization)

## ğŸ‡°ğŸ‡· ì†Œê°œ (Introduction - Korean)

ì´ í”„ë¡œì íŠ¸ëŠ” ê°•í™”í•™ìŠµ(Deep Reinforcement Learning)ì„ í™œìš©í•˜ì—¬ ê³µì¥ ë‚´ ì„¤ë¹„ ë°°ì¹˜ë¥¼ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•˜ëŠ” ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ëª©í‘œëŠ” ë‹¨ìˆœíˆ ì´ë™ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ì‹¤ì œ ìƒì‚° ëª©í‘œ(ì˜ˆ: ì‹œê°„ë‹¹ ìƒì‚°ëŸ‰)ì™€ ì„¤ë¹„ë³„ ì œì•½ ì¡°ê±´ê¹Œì§€ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ì„¤ë¹„ ë°°ì¹˜ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë ˆí¬ì§€í† ë¦¬ëŠ” ì•„ì´ë””ì–´ êµ¬ìƒë¶€í„° ì ì§„ì ìœ¼ë¡œ ê¸°ëŠ¥ì„ ê°œì„ í•´ ë‚˜ê°€ëŠ” í•™ìŠµ ì—¬ì •ì„ ê¸°ë¡í•©ë‹ˆë‹¤.

ì´ í”„ë¡œì íŠ¸ëŠ” í•™ìŠµ ë° í¬íŠ¸í´ë¦¬ì˜¤ ëª©ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ ì‚°ì—… í˜„ì¥ì— ë°”ë¡œ ì ìš©í•˜ê¸°ì—ëŠ” ë‹¨ìˆœí™”ëœ ëª¨ë¸ê³¼ ê°€ì •ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ‡¬ğŸ‡§ Introduction (ì†Œê°œ - English)

This project exploring how to optimize facility layouts within a factory using Deep Reinforcement Learning (DRL). The goal extends beyond simply minimizing travel distance to finding optimal equipment placements by considering actual production targets (e.g., throughput per hour) and machine-specific constraints. This repository records the learning process, from initial ideas to incremental feature improvements.

This project was developed for learning and portfolio purposes and includes simplified models and assumptions that may not be directly applicable to real-world industrial scenarios without further refinement.

---

## ğŸ‡°ğŸ‡· í”„ë¡œì íŠ¸ ì§„í–‰ ë‹¨ê³„ (Project Evolution - Korean)

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë³„ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê° Python íŒŒì¼ì€ í•´ë‹¹ ë‹¨ê³„ì˜ êµ¬í˜„ ë‚´ìš©ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

1.  **`01_random_search_layout.py`**:
    * ê¸°ë³¸ì ì¸ ì„¤ë¹„ ë°°ì¹˜ ë¬¸ì œ ì •ì˜.
    * ë¬´ì‘ìœ„ íƒìƒ‰(Random Search)ì„ í†µí•´ ì„¤ë¹„ ê°„ ì´ ì´ë™ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë ˆì´ì•„ì›ƒ íƒìƒ‰.
    * PyTorchë‚˜ ë”¥ëŸ¬ë‹ ì—†ì´ ìˆœìˆ˜ Pythonìœ¼ë¡œ êµ¬í˜„.

2.  **`02_pytorch_policy_gradient.py`**:
    * PyTorchë¥¼ ì²˜ìŒ ë„ì…í•˜ì—¬ ê°„ë‹¨í•œ ì •ì±… ê²½ì‚¬(Policy Gradient, REINFORCE ìœ ì‚¬) ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„.
    * ì‹ ê²½ë§ì´ ì„¤ë¹„ ë°°ì¹˜ ìœ„ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²°ì •í•˜ë„ë¡ í•™ìŠµ.
    * GPU ì‚¬ìš© ì§€ì› ì¶”ê°€.

3.  **`03_a2c_basic_layout.py`**:
    * ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ A2C(Advantage Actor-Critic)ë¡œ ê°œì„ .
    * ì•¡í„°(ì •ì±…ë§)ì™€ í¬ë¦¬í‹±(ê°€ì¹˜ë§)ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„± ë° íš¨ìœ¨ ì¦ëŒ€ ì‹œë„.
    * ì£¼ìš” ìµœì í™” ëª©í‘œëŠ” ì—¬ì „íˆ ì„¤ë¹„ ê°„ ì´ ì´ë™ ê±°ë¦¬ ìµœì†Œí™”.

4.  **`04_a2c_throughput_optimization.py`**:
    * A2C ì•Œê³ ë¦¬ì¦˜ì˜ ë³´ìƒ í•¨ìˆ˜ì— ì‹¤ì œ ìƒì‚°ì„± ì§€í‘œì¸ 'ì‹œê°„ë‹¹ ëª©í‘œ ìƒì‚°ëŸ‰(Throughput)' ë°˜ì˜.
    * ê° ì„¤ë¹„ì˜ ì‚¬ì´í´ íƒ€ì„ê³¼ ì„¤ë¹„ ê°„ ìì¬ ì´ë™ ì‹œê°„ì„ ê³ ë ¤í•˜ì—¬ ë¼ì¸ ìƒì‚°ëŸ‰ ì¶”ì •.
    * ì´ë™ ê±°ë¦¬ ìµœì†Œí™”ì™€ ìƒì‚°ëŸ‰ ëª©í‘œ ë‹¬ì„± ê°„ì˜ ê· í˜•ì„ ë§ì¶”ë„ë¡ í•™ìŠµ.

5.  **`05_a2c_constraints_optimization.py`**:
    * ë” í˜„ì‹¤ì ì¸ ì„¤ë¹„ ë°°ì¹˜ë¥¼ ìœ„í•´ ì„¤ë¹„ë³„ ì œì•½ ì¡°ê±´ ì¶”ê°€.
    * **ìµœì†Œ ì´ê²© ê±°ë¦¬ (Clearance)**: ì„¤ë¹„ ì£¼ë³€ì— í•„ìš”í•œ ìµœì†Œ ë¹ˆ ê³µê°„ì„ í•˜ë“œ ì œì•½ìœ¼ë¡œ ì ìš©.
    * **ë²½ ê·¼ì ‘ ì„ í˜¸ (Wall Affinity)**: íŠ¹ì • ì„¤ë¹„ê°€ ë²½ì— ê°€ê¹ê²Œ ë°°ì¹˜ë˜ë„ë¡ ìœ ë„ (ì†Œí”„íŠ¸ ì œì•½ - ë³´ìƒ í˜ë„í‹°).

6.  **`06_ppo_constraints_optimization.py`**:
    * ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ PPO(Proximal Policy Optimization)ë¡œ í•œ ë‹¨ê³„ ë” ë°œì „.
    * PPOì˜ í´ë¦¬í•‘ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í•™ìŠµ ì•ˆì •ì„± ë° ìƒ˜í”Œ íš¨ìœ¨ì„± í–¥ìƒ ê¸°ëŒ€.
    * ìƒì‚°ëŸ‰ ëª©í‘œ ë° ì„¤ë¹„ ì œì•½ ì¡°ê±´ì€ ê³„ì† ìœ ì§€í•˜ë©° ìµœì í™” ìˆ˜í–‰.

## ğŸ‡¬ğŸ‡§ Project Evolution (í”„ë¡œì íŠ¸ ì§„í–‰ ë‹¨ê³„ - English)

This project was developed iteratively through the following stages. Each Python file contains the implementation for that stage.

1.  **`01_random_search_layout.py`**:
    * Defines the basic facility layout problem.
    * Explores layouts to minimize total material travel distance using Random Search.
    * Implemented in pure Python without PyTorch or deep learning.

2.  **`02_pytorch_policy_gradient.py`**:
    * Introduces PyTorch by implementing a simple Policy Gradient (REINFORCE-like) algorithm.
    * Trains a neural network to sequentially decide machine placement positions.
    * Adds support for GPU usage.

3.  **`03_a2c_basic_layout.py`**:
    * Upgrades the reinforcement learning algorithm to A2C (Advantage Actor-Critic).
    * Attempts to improve learning stability and efficiency using an actor (policy network) and a critic (value network).
    * The primary optimization objective remains minimizing total travel distance between machines.

4.  **`04_a2c_throughput_optimization.py`**:
    * Incorporates a key productivity metric, 'target throughput per hour,' into the A2C algorithm's reward function.
    * Estimates line throughput by considering machine cycle times and material travel times between machines.
    * Trains the agent to balance minimizing travel distance with achieving production targets.

5.  **`05_a2c_constraints_optimization.py`**:
    * Adds machine-specific constraints for more realistic layouts.
    * **Clearance**: Enforces minimum empty space around machines as a hard constraint.
    * **Wall Affinity**: Encourages certain machines to be placed near walls as a soft constraint (via reward penalties).

6.  **`06_ppo_constraints_optimization.py`**:
    * Further advances the reinforcement learning algorithm to PPO (Proximal Policy Optimization).
    * Aims to improve learning stability and sample efficiency using PPO's clipping mechanism.
    * Continues to optimize for throughput targets and machine constraints.

---

## ğŸ‡°ğŸ‡· ì‹¤í–‰ ë°©ë²• (How to Run - Korean)
https://github.com/imjeasung/-Reinforcement-Learning-Facility-Layout-Optimization.git
1.  ì´ ë ˆí¬ì§€í† ë¦¬ë¥¼ ë¡œì»¬ ì»´í“¨í„°ì— í´ë¡ í•©ë‹ˆë‹¤:
    ```bash
    git clone https://github.com/imjeasung/-Reinforcement-Learning-Facility-Layout-Optimization.git
    cd YOUR_REPOSITORY_NAME
    ```
2.  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. (Python 3.8+ ê¶Œì¥)
    ```bash
    pip install torch numpy matplotlib
    ```
    (PyTorch ì„¤ì¹˜ ì‹œ, ì‚¬ìš©ìì˜ CUDA ë²„ì „ì— ë§ëŠ” ëª…ë ¹ì–´ë¥¼ [PyTorch ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://pytorch.org/)ì—ì„œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.)
3.  ê° ë‹¨ê³„ë³„ Python íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê°€ì¥ ìµœì‹  ë²„ì „ì¸ PPO ì•Œê³ ë¦¬ì¦˜ì„ ì‹¤í–‰í•˜ë ¤ë©´:
    ```bash
    python 06_ppo_constraints_optimization.py
    ```
4.  ìŠ¤í¬ë¦½íŠ¸ ë‚´ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°(ì˜ˆ: `num_episodes`, `learning_rate` ë“±)ë¥¼ ì¡°ì •í•˜ì—¬ í•™ìŠµ ê³¼ì •ì„ ì‹¤í—˜í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ‡¬ğŸ‡§ How to Run (ì‹¤í–‰ ë°©ë²• - English)

1.  Clone this repository to your local machine:
    ```bash
    git clone [https://github.com/YOUR_USERNAME/YOUR_REPOSITORY_NAME.git](https://github.com/YOUR_USERNAME/YOUR_REPOSITORY_NAME.git)
    cd YOUR_REPOSITORY_NAME
    ```
2.  Install the required libraries. (Python 3.7+ recommended)
    ```bash
    pip install torch numpy matplotlib
    ```
    (When installing PyTorch, it's advisable to check the [official PyTorch website](https://pytorch.org/) for commands specific to your CUDA version if you plan to use a GPU.)
3.  You can run each Python file to observe the results of that stage. For example, to run the latest PPO version:
    ```bash
    python 06_ppo_constraints_optimization.py
    ```
4.  You can adjust hyperparameters within the scripts (e.g., `num_episodes`, `learning_rate`) to experiment with the learning process.

---

## ğŸ‡°ğŸ‡· í–¥í›„ ê°œì„  ë°©í–¥ (Potential Future Work - Korean)

* ë” ë³µì¡í•˜ê³  ë‹¤ì–‘í•œ ì„¤ë¹„ ì œì•½ ì¡°ê±´ ì¶”ê°€ (ì˜ˆ: íŠ¹ì • ì„¤ë¹„ ê°„ ì¸ì ‘/ì´ê²© ì¡°ê±´).
* ìƒì‚° ë¼ì¸ì˜ ë³€ë™ì„±(ì„¤ë¹„ ê³ ì¥, ì‘ì—… ì‹œê°„ ë³€ë™ ë“±)ì„ ê³ ë ¤í•œ ì‹œë®¬ë ˆì´ì…˜ ë° ìµœì í™”.
* ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤(UI) ê°œë°œì„ í†µí•œ ì‚¬ìš© í¸ì˜ì„± ì¦ëŒ€.
* ë‹¤ë¥¸ ìµœì‹  ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì ìš© ë° ë¹„êµ.
* í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìë™í™”.

## ğŸ‡¬ğŸ‡§ Potential Future Work (í–¥í›„ ê°œì„  ë°©í–¥ - English)

* Adding more complex and diverse machine constraints (e.g., proximity/exclusion zones between specific machines).
* Simulation and optimization considering production line variability (e.g., machine breakdowns, fluctuating task times).
* Developing a user interface (UI) for easier interaction.
* Applying and comparing other state-of-the-art reinforcement learning algorithms.
* Automating hyperparameter optimization.

---
